{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon tutorial\n",
    "**Note**: This notebook runs on Python 3.6.\n",
    "\n",
    "In this notebook we will show you:\n",
    "- how to deploy a pipeline that selects the data and sends it to the model for prediction to UbiOps within seconds\n",
    "- how to easily trigger the pipeline everyday and output the results\n",
    "\n",
    "For this example we will use a model trained on (parts of) the amazon review dataset as taken from https://nijianmo.github.io/amazon/index.html*. The model takes in a written review of a product and outputs a classification of whether the review will be positive (1) or negative (0). This model can be used to automatically monitor the reviews of certain products. With using UbiOps, we can deploy a pipeline that is triggered every day that predicts and monitors the review score of products. If the score drops below a certain threshold, a signal is send, indicating that there may be something wrong with the product. \n",
    "This mechanism can of course be extended to other use cases involving textual (sentiment) analysis.\n",
    "\n",
    "\n",
    "*Justifying recommendations using distantly-labeled reviews and fined-grained aspects\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "Empirical Methods in Natural Language Processing (EMNLP), 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a connection with your UbiOps environment\n",
    "Add your API token and project name. You can also adapt the deployment name and deployment version name or leave the default values. Afterwards we initialize the client library, which establishes the connection with UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "API_TOKEN = '<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>'\n",
    "PROJECT_NAME= '<INSERT PROJECT NAME IN YOUR ACCOUNT>'\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)\n",
    "api.service_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deployment configurations\n",
    "DATA_COLLECTOR_DEPLOYMENT='data-collector'\n",
    "AMAZON_REVIEW_DEPLOYMENT='amazon-review-model'\n",
    "DEPLOYMENT_VERSION='v1'\n",
    "deployments_list = [DATA_COLLECTOR_DEPLOYMENT, AMAZON_REVIEW_DEPLOYMENT]\n",
    "\n",
    "# Pipeline configurations\n",
    "PIPELINE_NAME = \"amazon-pipeline\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this entire notebook after filling in your access token, the pipeline will be deployed to your UbiOps (Free) environment. You can thus check your environment after running to explore. You can also check the individual steps in this notebook to see what was done exactly and how you can adapt it to your own use case.\n",
    "\n",
    "We recommend to run the cells step by step, as some cells can take a few minutes to finish. You can run everything in one go as well and it will work, just allow a few minutes for building the individual deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating a pipeline in UbiOps\n",
    "\n",
    "We will not focus in dept on the technicalities behind deploying a model or pipeline on UbiOps. However, we will show what the data collector deployment and amazon model's predict function looks like. You can then run the steps in this notebook to create the deployments, upload the code and connect them in a pipeline.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector\n",
    "\n",
    "The first step of a machine learning pipeline often consists of a data collecting step. In our case, the data collector will take care of that. Data can be collected by connecting to a database or other data storage. However, we will read the data from a csv file. To simulate a real-life use case as close as possible, we only want to collect data from the given day, or, if that's not given, today's day. In the function below you can see what happens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This section is taken directly from data-collector/deployment.py \n",
    "\n",
    "def return_days(n):\n",
    "    return datetime.strptime(n, '%Y-%m-%d %H:%M:%S').strftime('%A')\n",
    "\n",
    "\n",
    "class InvalidDateError(Exception):\n",
    "\n",
    "    def __init__(self, day):\n",
    "        self.message = f\"No reviews found for day {day}\"\n",
    "\n",
    "\n",
    "def request(self, data):\n",
    "\n",
    "    # Get today's day\n",
    "    day = os.environ['DAY']  # This will be given through an environment variable\n",
    "    if not day:\n",
    "        day = datetime.today().strftime('%A')\n",
    "    day = day.title()\n",
    "\n",
    "    # Raise an error if the day is invalid (there's no data for saturday/sunday)\n",
    "    if day not in ['Monday', 'Tuesday', 'Wednesday', 'Thursday' 'Friday']:\n",
    "        raise InvalidDateError(day=day)\n",
    "\n",
    "    # Read in the reviews as a dataframe\n",
    "    reviews = pd.read_csv('reviews.csv')\n",
    "    \n",
    "    # Reformat the time of review into day_of_week so we can filter\n",
    "    reviews['day_of_week'] = reviews['time_of_review'].apply(return_days)\n",
    "    \n",
    "    # Only collect data from `day`\n",
    "    logger.info(f\"Collecting data from {day}\")\n",
    "    day_reviews = reviews[reviews['day_of_week'] == day]\n",
    "\n",
    "    logger.info(f\"Data retrieved successfully, sending data to model\")\n",
    "    df_string = day_reviews.to_json()\n",
    "\n",
    "    # Output the resulting dataframe (in JSON) to the model\n",
    "    return df_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Amazon review model\n",
    "\n",
    "The next deployment in the pipeline consists of the Amazon review model. This deployment takes care of predicting the review score and monitoring the outputted scores. It will predict a review score (0 or 1) on each product in the received data, take the average of the predicted scores and compare those with a threshold score. The threshold is the average score of yesterday or a certain set threshold (if yesterday was a weekend day). If the difference between the scores is too big, it wil give a signal in the logs. Lastly, the model outputs some metadata and a csv file with name \"review_scores_{day}. csv\" which can be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is taken directly from amazon-review-model/deployment.py \n",
    "def _connect_api(api_token):\n",
    "    client = ubiops.ApiClient(\n",
    "        ubiops.Configuration(api_key={'Authorization': api_token}, host='https://api.ubiops.com/v2.1')\n",
    "    )\n",
    "    return ubiops.CoreApi(client)\n",
    "\n",
    "def request(self, data):\n",
    "\n",
    "    # Load the data of the day into a dataframe\n",
    "    df = pd.read_json(data)\n",
    "\n",
    "    # From the dataframe we can get the day\n",
    "    day = df['day_of_week'].iloc[0]\n",
    "\n",
    "    # Let's get those predictions and take the averages of all review scores\n",
    "    average_predictions = self.return_predictions(dataframe=df)\n",
    "\n",
    "    # Get yesterday's day\n",
    "    days = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "    index = days.index(day)\n",
    "    yester_day_index = index - 1\n",
    "    yesterday = days[yester_day_index]\n",
    "\n",
    "    # Then retrieve yesterday's average review scores\n",
    "    # If those are not here (because it was a weekend day),\n",
    "    # we will compare the predicted review score to a set threshold\n",
    "    review_scores = self.retrieve_review_scores(yesterday=yesterday)\n",
    "\n",
    "    below_threshold = 0\n",
    "    products_below_threshold = []\n",
    "\n",
    "    # Now we are ready to see if the review scores are stable\n",
    "    # For every product in the predictions, compare the average review score to the threshold score\n",
    "    for index, value in average_predictions.items():\n",
    "\n",
    "        if review_scores:\n",
    "            review_threshold = review_scores.loc[review_scores['product_id'] == index, 'predictions'].iloc[0]\n",
    "        else:\n",
    "            # Load in the minimal threshold for the review\n",
    "            review_threshold = float(os.environ['THRESHOLD'])\n",
    "            \n",
    "        # Prompt a signal if the value drops significantly (0.2) below the threshold\n",
    "        if value < (review_threshold - 0.2):\n",
    "            below_threshold += 1\n",
    "            product_title = df.loc[df['product_id'] == index, 'product_name'].iloc[0]\n",
    "            products_below_threshold.append(product_title, )\n",
    "            logger.error(f\"Product with id {index}, has not met its review standards\")\n",
    "\n",
    "    # Output the review_scores to a csv, so it will be available for the next request.\n",
    "    average_predictions.to_csv(f\"review_scores_{day.lower()}.csv\", index=True)\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        'total_products': len(average_predictions),\n",
    "        'below_threshold': below_threshold,\n",
    "        'products_below_threshold': products_below_threshold,\n",
    "        'review_scores': f\"review_scores_{day.lower()}.csv\"\n",
    "    }\n",
    "\n",
    "def return_predictions(self, dataframe):\n",
    "\n",
    "    # The test column of the data is the 'review' column\n",
    "    x_test = dataframe['review']\n",
    "\n",
    "    # Because our X test data is text, it has to be transformed to vectors to be able to make predictions on\n",
    "    x_test_transformed = self.count_vectorizer.transform(x_test)\n",
    "\n",
    "    # Let's predict and paste the resulting array right onto the existing dataframe\n",
    "    dataframe['predictions'] = self.model.predict(x_test_transformed)\n",
    "\n",
    "    # Generate a new series with the average of the predictions per product\n",
    "    return dataframe.groupby('product_id')['predictions'].mean()\n",
    "\n",
    "def retrieve_review_scores(self, yesterday):\n",
    "\n",
    "    # Connect to APi to retrieve the file from UbiOps\n",
    "    api = self._connect_api(api_token=os.environ['API_TOKEN'])\n",
    "\n",
    "    # If the review score of yesterday is there retrieve it from UbiOps\n",
    "    project_name = os.environ['PROJECT_NAME']\n",
    "    blobs_list = api.blobs_list(project_name=project_name)\n",
    "    review_scores = None\n",
    "\n",
    "    for blob in blobs_list:\n",
    "        if blob.filename == f\"review_scores_{yesterday.lower()}.csv\":\n",
    "            response = api.blobs_get(project_name=project_name, blob_id=str(blob.id))\n",
    "            review_scores = pd.read_csv(response)\n",
    "\n",
    "    return review_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two deployments\n",
    "\n",
    "Let's create our data collector and our predictor model in UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the data-collector deployment\n",
    "# A structured input without input fields specified, signals to UbiOps that the deployment is a connector (it retrieves data)\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DATA_COLLECTOR_DEPLOYMENT,\n",
    "    description='Collect data from file according to day',\n",
    "    input_type='structured',\n",
    "    output_type='plain',\n",
    "    input_fields=[],\n",
    "    output_fields=[],\n",
    "    labels={'demo': 'amazon-review-pipeline'}\n",
    ")\n",
    "\n",
    "api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)\n",
    "\n",
    "# Create the amazon-model deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=AMAZON_REVIEW_DEPLOYMENT,\n",
    "    description='Predicts on reviews of products, outputs if their review score is below a certain threshold',\n",
    "    input_type='plain',\n",
    "    output_type='structured',\n",
    "    input_fields=[], # Plain type data does not require defined fields\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='total_products',\n",
    "            data_type='int'\n",
    "        ),\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='below_threshold',\n",
    "            data_type='int'\n",
    "        ),\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='products_below_threshold',\n",
    "            data_type='array_string'\n",
    "        ),\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='review_scores',\n",
    "            data_type='blob'\n",
    "        )\n",
    "    ],\n",
    "    labels={'demo': 'amazon-review-pipeline'}\n",
    ")\n",
    "\n",
    "api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Environment variables\n",
    "The next step is to add some environment variables to the deployments. This allows us to give some global and/or secrect parameters to a deployment. We use environment variables to give a day (e.g. \"monday\") to our data-collector and to set a threshold for our amazon-review-model. We also give the API_TOKEN and PROJECT_NAME to the model, so it is able to connect to UbiOps to retrieve the review_scores of yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "api.deployment_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,        \n",
    "    deployment_name=DATA_COLLECTOR_DEPLOYMENT,\n",
    "    data= {\n",
    "      \"name\": \"DAY\",\n",
    "      \"value\": \"monday\",\n",
    "      \"secret\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "api.deployment_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,        \n",
    "    deployment_name=AMAZON_REVIEW_DEPLOYMENT,\n",
    "    data= {\n",
    "      \"name\": \"THRESHOLD\",\n",
    "      \"value\": \"0.7\",\n",
    "      \"secret\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting these environment variables allows the model to connect to \n",
    "# the UbiOps client libraries from within the model's code.\n",
    "api.deployment_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,        \n",
    "    deployment_name=AMAZON_REVIEW_DEPLOYMENT,\n",
    "    data= {\n",
    "      \"name\": \"API_TOKEN\",\n",
    "      \"value\": API_TOKEN,\n",
    "      \"secret\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "api.deployment_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,        \n",
    "    deployment_name=AMAZON_REVIEW_DEPLOYMENT,\n",
    "    data= {\n",
    "      \"name\": \"PROJECT_NAME\",\n",
    "      \"value\": PROJECT_NAME,\n",
    "      \"secret\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions\n",
    "It is possible to create multiple versions of the same deployment. Let's create the first version and deploy the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version for both deployments\n",
    "version_template = ubiops.VersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    language='python3.6',\n",
    "    memory_allocation=1024,\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = the model will wait for 30 minutes after the last request\n",
    ")\n",
    "\n",
    "for deployment_name in deployments_list:\n",
    "    api.versions_create(\n",
    "        project_name=PROJECT_NAME,\n",
    "        deployment_name=deployment_name,\n",
    "        data=version_template\n",
    "    )\n",
    "\n",
    "    # Upload a zipped deployment package\n",
    "    file_upload_result =api.revisions_file_upload(\n",
    "        project_name=PROJECT_NAME,\n",
    "        deployment_name=deployment_name,\n",
    "        version=DEPLOYMENT_VERSION,\n",
    "        file=shutil.make_archive(f\"{deployment_name}_package\", 'zip', '.', deployment_name)\n",
    "    )\n",
    "\n",
    "    # Status of the version will be building\n",
    "    version_status = api.versions_get(       \n",
    "        project_name=PROJECT_NAME,        \n",
    "        deployment_name=deployment_name,        \n",
    "        version=DEPLOYMENT_VERSION    \n",
    "    )    \n",
    "    print(version_status.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models will now have been deployed to your UbiOps environment. Go ahead and take a look in the UI in the tab deployments to see it for yourself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching the deployments in a pipeline\n",
    "\n",
    "Now that we have both deployments and all environment variables ready, we can create a pipeline and add our deployment objects to them. For this, first both deployments need to have passed the building stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "status1 = 'building'\n",
    "status2 = 'building'\n",
    "while (status1 not in ['available', 'unavailable']) or (status2 not in ['available', 'unavailable']):    \n",
    "    version_status1 = api.versions_get(       \n",
    "        project_name=PROJECT_NAME,        \n",
    "        deployment_name=DATA_COLLECTOR_DEPLOYMENT,        \n",
    "        version=DEPLOYMENT_VERSION    \n",
    "    )    \n",
    "    status1 = version_status1.status\n",
    "    version_status2 = api.versions_get(       \n",
    "        project_name=PROJECT_NAME,        \n",
    "        deployment_name=AMAZON_REVIEW_DEPLOYMENT,        \n",
    "        version=DEPLOYMENT_VERSION    \n",
    "    )   \n",
    "    status2 = version_status2.status\n",
    "    sleep(1)\n",
    "\n",
    "print(f\"{DATA_COLLECTOR_DEPLOYMENT}: {status1}\")\n",
    "print(f\"{AMAZON_REVIEW_DEPLOYMENT}: {status2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The input of the pipeline is the same as the input of the first deployment (data-collector): structured type, without input fields\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=PIPELINE_NAME,\n",
    "    description='Pipeline that retrieves reviews of Amazon products '\n",
    "                'from a file and predicts the review score. It will '\n",
    "                'give a signal if a product does not meet its threshold review score.',\n",
    "    input_type='structured',\n",
    "    input_fields=[],\n",
    "    labels={'demo': 'amazon-review-pipeline'}\n",
    ")\n",
    "\n",
    "api.pipelines_create(project_name=PROJECT_NAME, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have a pipeline, now we just need to add our two components to it and connect it.\n",
    "\n",
    "**IMPORTANT**: If you get an error like: \"error\":\"Version is not available: The version is currently in the building stage\"\n",
    "Your model is not yet available and still building. \n",
    "Check in the UI if your model is ready and then rerun the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add both deployments to the pipeline\n",
    "for deployment_name in deployments_list:\n",
    "    object_template = ubiops.PipelineObjectCreate(\n",
    "        name=deployment_name,\n",
    "        reference_name=deployment_name,\n",
    "        version=DEPLOYMENT_VERSION\n",
    "    )\n",
    "    api.pipeline_objects_create(project_name=PROJECT_NAME, pipeline_name=PIPELINE_NAME, data=object_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Connecting the components\n",
    "\n",
    "# First connecting start --> data-collector\n",
    "pipeline_source = ubiops.AttachmentSourcesCreate(\n",
    "    source_name='pipeline_start', \n",
    "    mapping=[]\n",
    ")\n",
    "\n",
    "pipeline_attachment = ubiops.AttachmentsCreate(\n",
    "    destination_name=DATA_COLLECTOR_DEPLOYMENT, \n",
    "    sources=[pipeline_source]\n",
    ")\n",
    "\n",
    "api.pipeline_object_attachments_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    pipeline_name=PIPELINE_NAME, \n",
    "    data=pipeline_attachment\n",
    ")\n",
    "\n",
    "# Connection data-collector --> amazon-model\n",
    "pipeline_source = ubiops.AttachmentSourcesCreate(\n",
    "    source_name=DATA_COLLECTOR_DEPLOYMENT, \n",
    "    mapping=[]\n",
    ")\n",
    "\n",
    "pipeline_attachment = ubiops.AttachmentsCreate(\n",
    "    destination_name=AMAZON_REVIEW_DEPLOYMENT, \n",
    "    sources=[pipeline_source]\n",
    ")\n",
    "\n",
    "api.pipeline_object_attachments_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    pipeline_name=PIPELINE_NAME, \n",
    "    data=pipeline_attachment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request schedules \n",
    "Now that everything is set up, we can create a request schedule. Request schedules are handy when you want to kick off requests at certain specified times. They are easily set-up with a cron notation of the schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "schedule = ubiops.ScheduleCreate(\n",
    "    name=\"amazon-request\",\n",
    "    object_type=\"pipeline\",\n",
    "    object_name=PIPELINE_NAME,\n",
    "    schedule=\"0 8 * * 1-5\", # Run pipeline every working day Mo-Fri at 08:00 UTC (that is 09:00 for the Netherlands)\n",
    "    batch=True, # Set to True, so the requests + results will be stored for a week\n",
    "    enabled=False\n",
    ")\n",
    "api.request_schedules_create(project_name=PROJECT_NAME, data= schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "You can go ahead to the Web App and take a look in the user interface at what you have just built. If you want you can create a request to the pipeline with empty input, to see what happens.\n",
    "\n",
    "So there we have it! We have made a pipeline that gets triggered each day with a model that monitors it's own predictions. You can use this notebook as a reference for your own pipeline and use case. Just adapt the code in the deployment packages and alter the input and output fields as you wish and you should be good to go. \n",
    "\n",
    "For any questions, feel free to reach out to us via the customer service portal: https://ubiops.atlassian.net/servicedesk/customer/portals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
