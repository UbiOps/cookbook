{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit template\n",
    "**Note**: This notebook runs on Python 3.6.\n",
    "\n",
    "In this notebook we will show you the following:\n",
    "- how to make a training pipeline in UbiOps which preprocesses the data and trains and tests a model using scikit\n",
    "- how to make a production pipeline in UbiOps which takes in new data, processes it and feeds it to a trained model for prediction/classification\n",
    "\n",
    "For this example we will use a diabetes dataset from Kaggle to create a KNN classifier to predict if someone will have diabetes or not. Link to original dataset: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "If you run this entire notebook after filling in your access token, the two pipelines and all the necessary models will be deployed to your UbiOps environment. You can thus check your environment after running to explore. You can also check the individual steps in this notebook to see what we did exactly and how you can adapt it to your own use case.\n",
    "\n",
    "We recommend to run the cells step by step, as some cells can take a few minutes to finish. You can run everything in one go as well and it will work, just allow a few minutes for building the individual deployments.\n",
    "\n",
    "## Establishing a connection with your UbiOps environment\n",
    "Add your API token. Then we will provide a project name, deployment name and deployment version name. Afterwards we initialize the client library. This way we can deploy the two pipelines to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<INSERT PROJECT NAME IN YOUR ACCOUNT>\"\n",
    "DEPLOYMENT_NAME = 'data-preprocessor'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a training pipeline\n",
    "\n",
    "Our training pipeline will consist of two steps: preprocessing the data, and training a model.\n",
    "For each of these two steps we will create a separate deployment in UbiOps. This way the processing step can be reused later in the deployment pipeline (or in other pipelines) and each block will be scaled separately, increasing speed.\n",
    "\n",
    "### Preprocessing the data\n",
    "In the cell below the deployment.py of the preprocessing block is loaded. In the request function you can see that the deployment will clean up the data for further use and output that back in the form of two csv files. \n",
    "The deployment has the following input:\n",
    "- data: a csv file with the training data or with real data\n",
    "- training: a boolean indicating whether we using the data for training or not. In the case this boolean is set to true the target outcome is split of of the training data.\n",
    "\n",
    "The use of the boolean input \"training\" allows us to reuse this block later in a production pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load preprocessing_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - language (str): programming language the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising My Deployment\")\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Processing request for My Deployment\")\n",
    "        #Load the dataset\n",
    "        print(\"Loading data\")\n",
    "        diabetes_data = pd.read_csv(data[\"data\"])\n",
    "        \n",
    "        # The data contains some zero values which make no sense (like 0 skin thickness or 0 BMI). \n",
    "        # The following columns/variables have invalid zero values:\n",
    "        # glucosem bloodPressure, SkinThicknes, Insulin and BMI\n",
    "        # We will replace these zeros with NaN and after that we will replace them with a suitable value.\n",
    "        \n",
    "        print(\"Imputing missing values\")\n",
    "        # Replacing with NaN\n",
    "        diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n",
    "        # Imputing NaN\n",
    "        diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)\n",
    "        diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)\n",
    "        diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)\n",
    "        diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)\n",
    "        diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)\n",
    "        \n",
    "        # If this deployment is used for training, the target column\n",
    "        # needs to be split from the data\n",
    "        if data[\"training\"] == True:\n",
    "            X = diabetes_data.drop([\"Outcome\"], axis = 1) \n",
    "            y = diabetes_data.Outcome\n",
    "        else:\n",
    "            X = diabetes_data\n",
    "            y = pd.DataFrame([1])\n",
    "            \n",
    "            \n",
    "        print(\"Scaling data\")\n",
    "        # Since we are using a distance metric based algorithm we will use scikits standard scaler to scale all the features to [-1,1]\n",
    "        sc_X = StandardScaler()\n",
    "        X =  pd.DataFrame(sc_X.fit_transform(X,),\n",
    "                columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "               'BMI', 'DiabetesPedigreeFunction', 'Age'])\n",
    "        \n",
    "        # UbiOps expects JSON serializable output or files, so we convert the dataframes to csv\n",
    "        X.to_csv('X.csv', index = False)\n",
    "        y.to_csv('y.csv', index = False, header = False)\n",
    "\n",
    "        return {\n",
    "            \"cleaned_data\": 'X.csv', \"target_data\": 'y.csv'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a deployment and a deployment version for the package in the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='Clean up data',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='training',\n",
    "            data_type='bool',\n",
    "        )\n",
    "    ],\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='cleaned_data',\n",
    "            data_type='blob'\n",
    "        ),\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='target_data',\n",
    "            data_type='blob'\n",
    "        )\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.VersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    language='python3.6',\n",
    "    memory_allocation=512,\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = 30 minutes\n",
    ")\n",
    "\n",
    "api.versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('preprocessing_package', 'zip', '.', 'preprocessing_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='preprocessing_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model will now have been deployed to your UbiOps environment. Go ahead and take a look in the UI in the tab deployments to see it for yourself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "Now that we have the preprocessing deployment in UbiOps, we need a deployment that can take the output of the preprocessing step and train a KNN model on it. The code for this is in the \"training_package\" directory and can be seen in the next cell. We are going to perform the same steps as above to deploy this code in UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load training_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - language (str): programming language the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising My Deployment\")\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Processing request for My Deployment\")\n",
    "        # Load the dataset\n",
    "        print(\"Loading data\")\n",
    "        \n",
    "        X = pd.read_csv(data[\"cleaned_data\"])\n",
    "        y = pd.read_csv(data[\"target_data\"], header = None)\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42, stratify=y)\n",
    "\n",
    "        \n",
    "        # Setup a knn classifier with k neighbors\n",
    "        knn = KNeighborsClassifier(n_neighbors=7) \n",
    "        \n",
    "        # Fit the model on training data\n",
    "        knn.fit(X_train,y_train)\n",
    "        \n",
    "        # Get accuracy on test set. Note: In case of classification algorithms score method represents accuracy.\n",
    "        score = knn.score(X_test,y_test)\n",
    "        print('KNN accuracy: ' + str(score))\n",
    "        \n",
    "        # let us get the predictions using the classifier we had fit above\n",
    "        y_pred = knn.predict(X_test)\n",
    "                \n",
    "        # Output classification report\n",
    "        print('Classification report:')\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        \n",
    "        # Persisting the model for use in UbiOps\n",
    "        with open('knn.joblib', 'wb') as f:\n",
    "           dump(knn, 'knn.joblib')\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"trained_model\": 'knn.joblib', \"model_score\": score\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to deploy this step to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template_t = ubiops.DeploymentCreate(\n",
    "    name='model-training',\n",
    "    description='Trains a KNN model',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='cleaned_data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='target_data',\n",
    "            data_type='blob',\n",
    "        )\n",
    "    ],\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='trained_model',\n",
    "            data_type='blob'\n",
    "        ),\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='model_score',\n",
    "            data_type='double'\n",
    "        )\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template_t\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.VersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    language='python3.6',\n",
    "    memory_allocation=512,\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = 30 minutes\n",
    ")\n",
    "\n",
    "api.versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='model-training',\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('training_package', 'zip', '.', 'training_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='model-training',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='training_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if both deployments, preprocessing and training, are available for further use. Only once the models are built and ready can we use them in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "status1 = 'building'\n",
    "status2 = 'building'\n",
    "while (status1 != 'available' and 'failed' not in status1) or (status2 != 'available' and 'failed' not in status2) :    \n",
    "    version_status1 = api.versions_get(       \n",
    "        project_name=PROJECT_NAME,        \n",
    "        deployment_name=DEPLOYMENT_NAME,        \n",
    "        version=DEPLOYMENT_VERSION    \n",
    "    )    \n",
    "    status1 = version_status1.status\n",
    "    version_status2 = api.versions_get(       \n",
    "        project_name=PROJECT_NAME,        \n",
    "        deployment_name='model-training',        \n",
    "        version=DEPLOYMENT_VERSION    \n",
    "    )   \n",
    "    status2 = version_status2.status\n",
    "    sleep(1)\n",
    "    \n",
    "print(status1)\n",
    "print(status2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a training pipeline\n",
    "\n",
    "So right now we have two deployments: one cleaning up the input data and one using that data for training a model. We want to tie these two blocks together to create a workflow. We can use pipelines for that. Let's create a pipeline that takes the same input as the preprocessing block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_name = \"training-pipeline\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=training_pipeline_name,\n",
    "    description='A simple pipeline that cleans up data and trains a KNN model on it.',\n",
    "    input_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='training',\n",
    "            data_type='bool',\n",
    "        )\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.pipelines_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=pipeline_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pipeline, now we just need to add our two components to it and connect it.\n",
    "\n",
    "**IMPORTANT**: If you get an error like: \"error\":\"Version is not available: The version is currently in the building stage\"\n",
    "Your model is not yet available and still building. \n",
    "Check in the UI if your model is ready and then rerun the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the preprocessing deployment\n",
    "object_template = ubiops.PipelineObjectCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    reference_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION\n",
    ")\n",
    "api.pipeline_objects_create(project_name=PROJECT_NAME, pipeline_name=training_pipeline_name, data=object_template)\n",
    "\n",
    "# Adding the training deployment\n",
    "object_template2 = ubiops.PipelineObjectCreate(\n",
    "    name='model-training',\n",
    "    reference_name='model-training',\n",
    "    version=DEPLOYMENT_VERSION\n",
    ")\n",
    "api.pipeline_objects_create(project_name=PROJECT_NAME, pipeline_name=training_pipeline_name, data=object_template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the components\n",
    "\n",
    "# First connecting start --> preprocessor\n",
    "attachment_template1 = ubiops.AttachmentsCreate(\n",
    "    destination_name=DEPLOYMENT_NAME,\n",
    "    sources=[\n",
    "        ubiops.AttachmentSourcesCreate(\n",
    "            source_name='pipeline_start',\n",
    "            mapping=[\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='data',\n",
    "                    destination_field_name='data'\n",
    "                ),\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='training',\n",
    "                    destination_field_name='training'\n",
    "                )]\n",
    "        )]\n",
    ")\n",
    "\n",
    "api.pipeline_object_attachments_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    pipeline_name=PIPELINE_NAME, \n",
    "    data=attachment_template1\n",
    ")\n",
    "\n",
    "# Connection preprocessor --> model-training\n",
    "attachment_template2 = ubiops.AttachmentsCreate(\n",
    "    destination_name='model-training',\n",
    "    sources=[\n",
    "        ubiops.AttachmentSourcesCreate(\n",
    "            source_name=DEPLOYMENT_NAME,\n",
    "            mapping=[\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='cleaned_data',\n",
    "                    destination_field_name='cleaned_data'\n",
    "                ),\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='target_data',\n",
    "                    destination_field_name='target_data'\n",
    "                )]\n",
    "        )]\n",
    ")\n",
    "\n",
    "\n",
    "api.pipeline_object_attachments_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    pipeline_name=PIPELINE_NAME, \n",
    "    data=attachement_template2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline done!\n",
    "If you check in your UbiOps account under pipeline you will find a training-pipeline with our components in it and connected. Let's make a request to it. You can also make a request in the UI with the \"create direct request button\".\n",
    "\n",
    "this might take a while since the models will need a cold start as they have never been used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_name = \"training-pipeline\"\n",
    "\n",
    "blob = api.blobs_create(project_name=PROJECT_NAME, file='diabetes.csv', blob_ttl=1000)\n",
    "\n",
    "data = {'data': blob.id, 'training': True}\n",
    "pipeline_result = api.pipeline_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    pipeline_name=training_pipeline_name,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "print(pipeline_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's keep the blobid of the trained model safe for further use:\n",
    "# We will check the most recent blobs in our environment and look for the trained model one.\n",
    "# The trained model is kept in a joblib file called knn.joblib\n",
    "blobs_list = api.blobs_list(project_name = PROJECT_NAME)\n",
    "trained_model_blob = None\n",
    "for blob in blobs_list:\n",
    "    if blob.filename == 'knn.joblib':\n",
    "        trained_model_blob = str(blob.id)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with the trained model\n",
    "\n",
    "Our model is trained and ready. Now we still need to deploy a predictor to UbiOps that uses this model for predicting. \n",
    "\n",
    "I already have the code and the requirements ready that need to be deployed to UbiOps. However, the joblib file is still missing in this folder. We dont want to manually download the joblib file output from the training pipeline, but automatically put it in the deployment package for the predictor. After that we can zip up the folder and push it to UbiOps like we did with the previous two packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load predictor_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - language (str): programming language the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising KNN model\")\n",
    "\n",
    "        KNN_MODEL = os.path.join(base_directory, \"knn.joblib\")\n",
    "        self.model = load(KNN_MODEL)\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        input_data = pd.read_csv(data['data'])\n",
    "        \n",
    "        print(\"Prediction being made\")\n",
    "        prediction = self.model.predict(input_data)\n",
    "        diabetes_instances = sum(prediction)\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        pd.DataFrame(prediction).to_csv('prediction.csv', header = ['diabetes_prediction'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": 'prediction.csv', \"predicted_diabetes_instances\": diabetes_instances\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download the trained model joblib and put it in the predictor package directory\n",
    "base_directory = os.path.dirname(os.path.abspath(\"scikit-deployment\"))\n",
    "\n",
    "with api.blobs_get(project_name=PROJECT_NAME, blob_id=trained_model_blob) as response:\n",
    "    output_path = os.path.join(base_directory, \"predictor_package\", response.getfilename())\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(response.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to zip the deployment package\n",
    "shutil.make_archive('predictor_package', 'zip', '.', 'predictor_package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the KNN model\n",
    "The folder is ready, now we need to make a deployment in UbiOps. Just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name='knn-model',\n",
    "    description='KNN model for diabetes prediction',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data_cleaning_artefact',\n",
    "            data_type='blob',\n",
    "        )\n",
    "    ],\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='prediction',\n",
    "            data_type='blob'\n",
    "        ),\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='predicted_diabetes_instances',\n",
    "            data_type='int'\n",
    "        )\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.VersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    language='python3.6',\n",
    "    memory_allocation=512,\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = 30 minutes\n",
    ")\n",
    "\n",
    "api.versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='predictor_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the deployment is ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = 'building'\n",
    "while status != 'available' and 'failed' not in status:    \n",
    "    version_status = api.versions_get(       \n",
    "        project_name=PROJECT_NAME,        \n",
    "        deployment_name='knn-model',        \n",
    "        version=DEPLOYMENT_VERSION    \n",
    "    )    \n",
    "    status = version_status.status\n",
    "    sleep(1)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the production pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_pipeline_name = \"production-pipeline\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=prod_pipeline_name,\n",
    "    description=\"A simple pipeline that cleans up data and let's a KNN model predict on it.\",\n",
    "    input_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='training',\n",
    "            data_type='bool',\n",
    "        )\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.pipelines_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=pipeline_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the preprocessing and the predicting components and connecting them.\n",
    "\n",
    "**IMPORTANT**: If you get an error like: \"error\":\"Version is not available: The version is currently in the building stage\"\n",
    "Your model is not yet available and still building. \n",
    "Check in the UI if your model is ready and then rerun the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the preprocessing deployment\n",
    "object_template = ubiops.PipelineObjectCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    reference_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION\n",
    ")\n",
    "api.pipeline_objects_create(project_name=PROJECT_NAME, pipeline_name=prod_pipeline_name, data=object_template)\n",
    "\n",
    "# Adding the KNN deployment\n",
    "object_template2 = ubiops.PipelineObjectCreate(\n",
    "    name='knn-model',\n",
    "    reference_name='knn-model',\n",
    "    version=DEPLOYMENT_VERSION\n",
    ")\n",
    "api.pipeline_objects_create(project_name=PROJECT_NAME, pipeline_name=prod_pipeline_name, data=object_template2)\n",
    "\n",
    "# Connecting the components\n",
    "# First connecting start --> preprocessor\n",
    "attachment_template1 = ubiops.AttachmentsCreate(\n",
    "    destination_name=DEPLOYMENT_NAME,\n",
    "    sources=[\n",
    "        ubiops.AttachmentSourcesCreate(\n",
    "            source_name='pipeline_start',\n",
    "            mapping=[\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='data',\n",
    "                    destination_field_name='data'\n",
    "                ),\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='training',\n",
    "                    destination_field_name='training'\n",
    "                )]\n",
    "        )]\n",
    ")\n",
    "\n",
    "api.pipeline_object_attachments_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    pipeline_name=PIPELINE_NAME, \n",
    "    data=attachment_template1\n",
    ")\n",
    "\n",
    "# Connection preprocessor --> KNN model\n",
    "attachment_template2 = ubiops.AttachmentsCreate(\n",
    "    destination_name='knn-model',\n",
    "    sources=[\n",
    "        ubiops.AttachmentSourcesCreate(\n",
    "            source_name=DEPLOYMENT_NAME,\n",
    "            mapping=[\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='cleaned_data',\n",
    "                    destination_field_name='data'\n",
    "                ),\n",
    "                ubiops.AttachmentFieldsCreate(\n",
    "                    source_field_name='target_data',\n",
    "                    destination_field_name='data_cleaning_artefact'\n",
    "                )]\n",
    "        )]\n",
    ")\n",
    "\n",
    "\n",
    "api.pipeline_object_attachments_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    pipeline_name=PIPELINE_NAME, \n",
    "    data=attachement_template2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "You can go ahead to the Web App and take a look in the user interface at what you have just built. If you want you can create a request to the production pipeline using the \"dummy_data_for_predicting.csv\" and setting the \"training\" input to \"False\". The dummy data is just the diabetes data with the Outcome column chopped of. \n",
    "\n",
    "So there we have it! We have made a training pipeline and a production pipeline using the scikit learn library. You can use this notebook to base your own pipelines on. Just adapt the code in the deployment packages and alter the input and output fields as you wish and you should be good to go. \n",
    "\n",
    "For any questions, feel free to reach out to us via the customer service portal: https://ubiops.atlassian.net/servicedesk/customer/portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}