{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow template\n",
    "Note: This notebook runs on Python 3.6.\n",
    "\n",
    "In this notebook we will show you the following:\n",
    "\n",
    "How to create a deployment that uses a built tensorflow model to make predictions on the fuel efficiency of late-1970s and early 1980s automobiles or MPG (mile per gallon).\n",
    "\n",
    "This example uses the classic Auto MPG Dataset. [Link to the dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data)\n",
    "\n",
    "\n",
    "If you run this entire notebook after filling in your access token, the tensorflow deployment will be deployed to your UbiOps environment. You can thus check your environment after running to explore. You can also check the individual steps in this notebook to see what we did exactly and how you can adapt it to your own use case.\n",
    "\n",
    "We recommend to run the cells step by step, as some cells can take a few minutes to finish. You can run everything in one go as well and it will work, just allow a few minutes for building the individual deployments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a connection with your UbiOps environmentÂ¶\n",
    "\n",
    "Add your API token. Then we will provide a project name, deployment name and deployment version name. Afterwards we initialize the client library. This way we can deploy the deployment to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "API_TOKEN = \"<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<INSERT PROJECT NAME IN YOUR ACCOUNT>\"\n",
    "DEPLOYMENT_NAME = 'tensorflow-deployment'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "This example will be based on the [regression tutorial from tensorflow](https://www.tensorflow.org/tutorials/keras/regression#get_the_data).\n",
    "\n",
    "Since this document will be more focused on the deploying side of the ML process we will not go too in depth in the development of the model, so we will make use of the pre-trained model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first install the python packages we need for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install pandas --user\n",
    "!{sys.executable} -m pip install numpy --user\n",
    "!{sys.executable} -m pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# predict the fuel efficiency of late-1970s and early 1980s automobiles\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Load data\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv('./auto-mpg.csv', names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "dataset = raw_dataset.copy()\n",
    "\n",
    "# Drop all but the horsepower and mpg columns\n",
    "dataset = dataset[['Horsepower', 'MPG']]\n",
    "\n",
    "# Drop unknown value rows\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Split into train and test set 80-20\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "# Checking how our data structure looks like\n",
    "print('Data:')\n",
    "print(train_dataset.describe().transpose())\n",
    "\n",
    "# Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict.\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')\n",
    "\n",
    "\n",
    "# Create the horsepower Normalization layer:\n",
    "horsepower = np.array(train_features)\n",
    "horsepower_normalizer = preprocessing.Normalization(input_shape=[1,])\n",
    "horsepower_normalizer.adapt(horsepower)\n",
    "\n",
    "# Build the sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    horsepower_normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "\n",
    "# Configure training procedure\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')\n",
    "\n",
    "# Train the model using the prepared data\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=100,\n",
    "    # suppress logging\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = model.evaluate(\n",
    "    test_features,\n",
    "    test_labels,\n",
    "    verbose=0\n",
    "\n",
    ")\n",
    "\n",
    "print(f'mean absolute error of the model is is {mae}')\n",
    "\n",
    "# Save the model using h5 format for use within our deployment to make predictions\n",
    "# Note that this will save the model to the folder where our model is \n",
    "model.save(\"tensorflow_deployment_package/tensorflow_model.h5\")\n",
    "print('Model created and saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tensorflow deployment\n",
    "Now that we have our model saved it is time to create a deployment in UbiOps that will make use of it.\n",
    "\n",
    "In the cell below the deployment.py which will take the data we wish to predict the MPG for. As you can see in the initialization step we load the model we created earlier, then in the request method we make use of it to make a prediction. Input to this model is:\n",
    "\n",
    "* data: a csv file with the data to predict the MPG (mile per gallon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tensorflow_deployment_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - language (str): programming language the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising the model\")\n",
    "\n",
    "        model_file = os.path.join(base_directory, \"tensorflow_model.h5\")\n",
    "        self.model = load_model(model_file)\n",
    "\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        input_data = pd.read_csv(data['data'])\n",
    "        \n",
    "        print(\"Prediction being made\")\n",
    "        prediction = self.model.predict(input_data)\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        pd.DataFrame(prediction).to_csv('prediction.csv', header = ['MPG'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": 'prediction.csv',\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deploying to UbiOpsÂ¶\n",
    "\n",
    "Now we have all the pieces we need to create our deployment on UbiOps. In the cell below as you see a deployment is being created, then a version of the deployment is created and the deployment code is zipped and uploaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='Tensorflow deployment',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "    ],\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='prediction',\n",
    "            data_type='blob'\n",
    "        ),\n",
    "    ],\n",
    "    labels={\"demo\": \"tensorflow\"}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.VersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    language='python3.6',\n",
    "    memory_allocation=512,\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = 30 minutes\n",
    ")\n",
    "\n",
    "api.versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('tensorflow_deployment_package', 'zip', '.', 'tensorflow_deployment_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='tensorflow_deployment_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "You can go ahead to the Web App and take a look in the user interface at what you have just built. If you want you can create a request to the Tensorflow deployment using the \"dummy_data_to_predict.csv\". The dummy data is just the horsepower data. \n",
    "\n",
    "So there we have it! We have created a deployment and using the tensorflow  library. You can use this notebook to base your own deployments on. Just adapt the code in the deployment packages and alter the input and output fields as you wish and you should be good to go. \n",
    "\n",
    "For any questions, feel free to reach out to us via the customer service portal: https://ubiops.atlassian.net/servicedesk/customer/portals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}